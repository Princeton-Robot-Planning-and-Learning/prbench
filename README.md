# PRBench

![workflow](https://github.com/Princeton-Robot-Planning-and-Learning/prbench/actions/workflows/ci.yml/badge.svg)

A **p**hysical **r**easoning **bench**mark for robotics.

For currently implemented environments, see `docs/envs`.

## :clock1: Status

To even call this "pre-alpha" would be generous. Unless you have had a direct conversation with a maintainer, this code is not ready for you! But check back soon.

## :airplane: Overview

This is a challenging benchmark for task and motion planning (TAMP), reinforcement learning, robot foundation models, and any combination of the previous three. Our main goals:

1. Better communicate what makes TAMP hard and interesting.
2. Make it easier to directly compare different methods.
3. Create challenge problems that we (the benchmark creators) can work towards solving over the next several years.

The benchmark includes a diverse collection of environments. Each environment will have **diverse task distributions**, which will typically be procedurally generated. Tasks will have **tricky constraints**, **long horizons**, **sparse feedback**, and **continuous spaces**.

## :triangular_ruler: Design Principles

**Broad view of TAMP.** We are interested in the kinds of tasks that are often studied in TAMP, but we will not commit to a rigid definition of TAMP. If an environment features tricky constraints, long horizons, sparse feedback, and continuous spaces, then it is a good fit for PRBench.

**BYOM: Bring Your Own Models**. We expect TAMP planners to use various environment models (operators, predicates, samplers, etc.). These models will not be provided in PRBench, but users are welcome to hand-design environment-specific models. Many of the PRBench environments will be sufficiently difficult that solving them by any means (including any form of manual engineering) will be an accomplishment.

**Flexible specification.** We will not make any universal assumptions about object-centric spaces (although many environments will have these) or transitions (although many environments will use the same simulators). To apply one method across environments with different specifications, researchers may manually or automatically generate interfaces that convert observations or actions into their expected format.

**Easy to install and contribute.** We will make sure that all environments are easy to install and use across platforms. We will stay compatible with the Gym API. We will also define a clear and friendly guide for contributing new environments.

**Community buy-in**. We will solicit feedback throughout the development of PRBench to ensure that we are building something that people want.


## :heavy_plus_sign: Contributing

### :ballot_box_with_check: Requirements
1. Python 3.10+
2. Tested on MacOS Catalina and Ubuntu 22.04 (but we aim to support most platforms)

### :wrench: Installation
1. Recommended: create and source a virtualenv (perhaps with [uv](https://github.com/astral-sh/uv))
2. `pip install -e ".[develop]"`
3. `pre-commit install`

### :microscope: Check Installation
Run `./run_ci_checks.sh`. It should complete with all green successes.

### :new: Adding New Environments
Detailed instructions to be added soon. For now, please see the examples in `src/prbench/env`. Also consider:
* Environments are registered in `src/prbench/__init__.py`
* Documentation is autogenerated with `scripts/generate_env_docs.py`, which is triggered on precommit (see `.pre-commit-config.yaml`)
* Code must go through the pull request review process
* All checks must pass before code is merged (see `./run_ci_checks.sh`)
