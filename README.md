# PRBench

![workflow](https://github.com/Princeton-Robot-Planning-and-Learning/prbench/actions/workflows/ci.yml/badge.svg)

A **p**hysical **r**easoning **bench**mark for robotics.

There's growing excitement around large language models and their ability to "reason"â€”but reasoning isn't just about tokens and text. **Robots must reason too**: over long horizons, under uncertainty, and with sparse feedback. And unlike purely symbolic systems, **robotic reasoning is physical**: it's grounded in low-level, continuous state and action spaces. It requires understanding kinematics, geometry, dynamics, contact, force, tool use, and more.

This benchmark is designed for this kind of physical reasoning with robots. We invite researchers to try their best task and motion planning, reinforcement learning, imitation learning, and foundation models approaches. We hope that PRBench will bridge perspectives and foster shared progress toward physically intelligent robots.

## :clock1: Status

For currently implemented environments, see `docs/envs`.

Unless you have had a direct conversation with a maintainer, this code is not ready for you! But check back soon.

## :zap: Usage Example

```python
import prbench
prbench.register_all_environments()
env = prbench.make("prbench/Obstruction2D-o3-v0")  # 3 obstructions
obs, info = env.reset()  # procedural generation
action = env.action_space.sample()
next_obs, reward, terminated, truncated, info = env.step(action)
img = env.render()  
```

## :muscle: Challenges for Existing Approaches

What makes PRBench challenging?

### For Reinforcement Learning

Environments have long horizons and sparse rewards. Users are welcome to engineer dense rewards, but doing so may be nontrivial. Environments also have very diverse task distributions (as in the `reset()` function), so learned policies must generalize.

### For Imitation Learning

As with RL, generalization across tasks is a major challenge for imitation learning. Furthermore, we supply some demonstrations, but they are typically suboptimal, multimodal, and limited in quantity. Users are welcome to collect their own demonstrations.

### For Language Models

The physical reasoning required in PRBench is not easy to represent in natural language alone. Vision-language and vision-language-action models may fare better, but the tasks in PRBench are beyond the capabilities of current VLMs and VLAs.* (*This is an empirical claim that we will test!)

### For Hierarchical Approaches

Approaches that first decide "what to do" and then decide "how to do it" will run into difficulties in PRBench when there are couplings between these high-level and low-level decisions. For example, the exact grasp of an object may determine whether the object can later be placed into a tight space.

### For Task and Motion Planning

PRBench does not provide any models for TAMP. Users are welcome to engineer their own, but doing so may be nontrivial. Furthermore, some environments in PRBench are meant to strain the assumptions that are sometimes made in TAMP. Finally, some environments contain many objects, which may make planning slow even when models are available.

## :octocat: Contributing

### :ballot_box_with_check: Requirements
1. Python 3.10+
2. Tested on MacOS Monterey and Ubuntu 22.04 (but we aim to support most platforms)

### :wrench: Installation
1. Recommended: create and source a virtualenv (perhaps with [uv](https://github.com/astral-sh/uv))
2. `pip install -e ".[develop]"`
3. `pre-commit install`

### :microscope: Check Installation
Run `./run_ci_checks.sh`. It should complete with all green successes.

### :mag: General Guidelines
* All checks must pass before code is merged (see `./run_ci_checks.sh`)
* All code goes through the pull request review process

### :new: Adding New Environments
Some new environment requests are in Issues. To add a new environment, please see the examples in `src/prbench/env`. Also consider:
* Environments are registered in `src/prbench/__init__.py`
* Documentation is autogenerated with `scripts/generate_env_docs.py`, which is triggered on precommit (see `.pre-commit-config.yaml`)
* Each environment should have at least one demonstration (see `scripts/collect_demos.py`)
* After collecting a demonstraction, create a video with `scripts/generate_demo_video.py`, which will be used in the autogenerated documentation

### :video_camera: Demonstration Collection
Here is an example of demo collection.
<video src='https://github.com/user-attachments/assets/265b0401-6615-47be-8fca-cb9f409b6945' />

### :oncoming_automobile: Roadmap
For specific environments, we will use issue tracking. Here are higher level TODOs:

- [ ] Decide which simulator(s) to use for 3D environments
- [ ] Determine what metrics we want to record, and how
- [ ] Run reinforcement learning baselines
- [ ] Run VLM/VLA baselines
- [ ] Create interface to PDDLStream
- [ ] Create interface to "task then motion planning"
- [ ] Run "code as policies" type baselines
- [ ] Collect at least 100 demonstrations per environment
- [ ] Run imitation learning baselines
- [ ] Create website
